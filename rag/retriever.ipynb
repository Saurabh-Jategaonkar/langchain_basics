{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retriever and Chain with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '0.00-51177066' : FloatObject (b'0.00-51177066') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-60790265' : FloatObject (b'0.00-60790265') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-56221883' : FloatObject (b'0.00-56221883') invalid; use 0.0 instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\nJunyuan Xie JXIE@CS.WASHINGTON .EDU\\nUniversity of Washington\\nRoss Girshick RBG@FB.COM\\nFacebook AI Research (FAIR)\\nAli Farhadi ALI@CS.WASHINGTON .EDU\\nUniversity of Washington\\nAbstract\\nClustering is central to many data-driven appli-\\ncation domains and has been studied extensively\\nin terms of distance functions and grouping al-\\ngorithms. Relatively little work has focused on\\nlearning representations for clustering. In this\\npaper, we propose Deep Embedded Clustering\\n(DEC), a method that simultaneously learns fea-\\nture representations and cluster assignments us-\\ning deep neural networks. DEC learns a map-\\nping from the data space to a lower-dimensional\\nfeature space in which it iteratively optimizes a\\nclustering objective. Our experimental evalua-\\ntions on image and text corpora show signiﬁcant\\nimprovement over state-of-the-art methods.\\n1. Introduction\\nClustering, an essential data analysis and visualization\\ntool, has been studied extensively in unsupervised machine\\nlearning from different perspectives: What deﬁnes a clus-\\nter? What is the right distance metric? How to efﬁciently\\ngroup instances into clusters? How to validate clusters?\\nAnd so on. Numerous different distance functions and em-\\nbedding methods have been explored in the literature. Rel-\\natively little work has focused on the unsupervised learning\\nof the feature space in which to perform clustering.\\nA notion of distance ordissimilarity is central to data clus-\\ntering algorithms. Distance, in turn, relies on represent-\\ning the data in a feature space. The k-means cluster-\\ning algorithm (MacQueen et al., 1967), for example, uses\\nthe Euclidean distance between points in a given feature\\nspace, which for images might be raw pixels or gradient-\\nProceedings of the 33rdInternational Conference on Machine\\nLearning , New York, NY , USA, 2016. JMLR: W&CP volume\\n48. Copyright 2016 by the author(s).orientation histograms. The choice of feature space is cus-\\ntomarily left as an application-speciﬁc detail for the end-\\nuser to determine. Yet it is clear that the choice of feature\\nspace is crucial; for all but the simplest image datasets,\\nclustering with Euclidean distance on raw pixels is com-\\npletely ineffective. In this paper, we revisit cluster analysis\\nand ask: Can we use a data driven approach to solve for\\nthe feature space and cluster memberships jointly?\\nWe take inspiration from recent work on deep learning for\\ncomputer vision (Krizhevsky et al., 2012; Girshick et al.,\\n2014; Zeiler & Fergus, 2014; Long et al., 2014), where\\nclear gains on benchmark tasks have resulted from learn-\\ning better features. These improvements, however, were\\nobtained with supervised learning, whereas our goal is un-\\nsupervised data clustering. To this end, we deﬁne a pa-\\nrameterized non-linear mapping from the data space Xto\\na lower-dimensional feature space Z, where we optimize\\na clustering objective. Unlike previous work, which oper-\\nates on the data space or a shallow linear embedded space,\\nwe use stochastic gradient descent (SGD) via backpropaga-\\ntion on a clustering objective to learn the mapping, which\\nis parameterized by a deep neural network. We refer to\\nthis clustering algorithm as Deep Embedded Clustering , or\\nDEC.\\nOptimizing DEC is challenging. We want to simultane-\\nously solve for cluster assignment and the underlying fea-\\nture representation. However, unlike in supervised learn-\\ning, we cannot train our deep network with labeled data.\\nInstead we propose to iteratively reﬁne clusters with an\\nauxiliary target distribution derived from the current soft\\ncluster assignment. This process gradually improves the\\nclustering as well as the feature representation.\\nOur experiments show signiﬁcant improvements over state-\\nof-the-art clustering methods in terms of both accuracy and\\nrunning time on image and textual datasets. We evaluate\\nDEC on MNIST (LeCun et al., 1998), STL (Coates et al.,arXiv:1511.06335v2  [cs.LG]  24 May 2016', metadata={'source': 'sample_pdf.pdf', 'page': 0}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\n2011), and REUTERS (Lewis et al., 2004), comparing it\\nwith standard and state-of-the-art clustering methods (Nie\\net al., 2011; Yang et al., 2010). In addition, our experiments\\nshow that DEC is signiﬁcantly less sensitive to the choice\\nof hyperparameters compared to state-of-the-art methods.\\nThis robustness is an important property of our clustering\\nalgorithm since, when applied to real data, supervision is\\nnot available for hyperparameter cross-validation.\\nOur contributions are: (a) joint optimization of deep em-\\nbedding and clustering; (b) a novel iterative reﬁnement\\nvia soft assignment; and (c) state-of-the-art clustering re-\\nsults in terms of clustering accuracy and speed. Our Caffe-\\nbased (Jia et al., 2014) implementation of DEC is available\\nathttps://github.com/piiswrong/dec .\\n2. Related work\\nClustering has been extensively studied in machine learn-\\ning in terms of feature selection (Boutsidis et al., 2009; Liu\\n& Yu, 2005; Alelyani et al., 2013), distance functions (Xing\\net al., 2002; Xiang et al., 2008), grouping methods (Mac-\\nQueen et al., 1967; V on Luxburg, 2007; Li et al., 2004),\\nand cluster validation (Halkidi et al., 2001). Space does\\nnot allow for a comprehensive literature study and we refer\\nreaders to (Aggarwal & Reddy, 2013) for a survey.\\nOne branch of popular methods for clustering is k-\\nmeans (MacQueen et al., 1967) and Gaussian Mixture\\nModels (GMM) (Bishop, 2006). These methods are fast\\nand applicable to a wide range of problems. However, their\\ndistance metrics are limited to the original data space and\\nthey tend to be ineffective when input dimensionality is\\nhigh (Steinbach et al., 2004).\\nSeveral variants of k-means have been proposed to address\\nissues with higher-dimensional input spaces. De la Torre &\\nKanade (2006); Ye et al. (2008) perform joint dimension-\\nality reduction and clustering by ﬁrst clustering the data\\nwithk-means and then projecting the data into a lower di-\\nmensions where the inter-cluster variance is maximized.\\nThis process is repeated in EM-style iterations until conver-\\ngence. However, this framework is limited to linear embed-\\nding; our method employs deep neural networks to perform\\nnon-linear embedding that is necessary for more complex\\ndata.\\nSpectral clustering and its variants have gained popular-\\nity recently (V on Luxburg, 2007). They allow more ﬂex-\\nible distance metrics and generally perform better than k-\\nmeans. Combining spectral clustering and embedding has\\nbeen explored in Yang et al. (2010); Nie et al. (2011). Tian\\net al. (2014) proposes an algorithm based on spectral clus-\\ntering, but replaces eigenvalue decomposition with deep\\nautoencoder, which improves performance but further in-\\ncreases memory consumption.Most spectral clustering algorithms need to compute the\\nfull graph Laplacian matrix and therefore have quadratic\\nor super quadratic complexities in the number of data\\npoints. This means they need specialized machines with\\nlarge memory for any dataset larger than a few tens of\\nthousands of points. In order to scale spectral clustering\\nto large datasets, approximate algorithms were invented to\\ntrade off performance for speed (Yan et al., 2009). Our\\nmethod, however, is linear in the number of data points and\\nscales gracefully to large datasets.\\nMinimizing the Kullback-Leibler (KL) divergence be-\\ntween a data distribution and an embedded distribution has\\nbeen used for data visualization and dimensionality reduc-\\ntion (van der Maaten & Hinton, 2008). T-SNE, for instance,\\nis a non-parametric algorithm in this school and a paramet-\\nric variant of t-SNE (van der Maaten, 2009) uses deep neu-\\nral network to parametrize the embedding. The complexity\\nof t-SNE is O(n2), wherenis the number of data points,\\nbut it can be approximated in O(nlogn)(van Der Maaten,\\n2014).\\nWe take inspiration from parametric t-SNE. Instead of min-\\nimizing KL divergence to produce an embedding that is\\nfaithful to distances in the original data space, we deﬁne\\na centroid-based probability distribution and minimize its\\nKL divergence to an auxiliary target distribution to simul-\\ntaneously improve clustering assignment and feature repre-\\nsentation. A centroid-based method also has the beneﬁt of\\nreducing complexity to O(nk), wherekis the number of\\ncentroids.\\n3. Deep embedded clustering\\nConsider the problem of clustering a set of npoints{xi∈\\nX}n\\ni=1intokclusters, each represented by a centroid\\nµj,j= 1,...,k . Instead of clustering directly in the data\\nspaceX, we propose to ﬁrst transform the data with a non-\\nlinear mapping fθ:X→Z, whereθare learnable pa-\\nrameters and Zis the latent feature space . The dimen-\\nsionality of Zis typically much smaller than Xin order\\nto avoid the “curse of dimensionality” (Bellman, 1961). To\\nparametrize fθ, deep neural networks (DNNs) are a natu-\\nral choice due to their theoretical function approximation\\nproperties (Hornik, 1991) and their demonstrated feature\\nlearning capabilities (Bengio et al., 2013).\\nThe proposed algorithm (DEC) clusters data by simultane-\\nously learning a set of kcluster centers{µj∈Z}k\\nj=1in the\\nfeature space Zand the parameters θof the DNN that maps\\ndata points into Z. DEC has two phases: (1) parameter ini-\\ntialization with a deep autoencoder (Vincent et al., 2010)\\nand (2) parameter optimization (i.e., clustering), where we\\niterate between computing an auxiliary target distribution\\nand minimizing the Kullback–Leibler (KL) divergence to', metadata={'source': 'sample_pdf.pdf', 'page': 1}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\nit. We start by describing phase (2) parameter optimiza-\\ntion/clustering, given an initial estimate of θand{µj}k\\nj=1.\\n3.1. Clustering with KL divergence\\nGiven an initial estimate of the non-linear mapping fθand\\nthe initial cluster centroids {µj}k\\nj=1, we propose to im-\\nprove the clustering using an unsupervised algorithm that\\nalternates between two steps. In the ﬁrst step, we com-\\npute a soft assignment between the embedded points and\\nthe cluster centroids. In the second step, we update the\\ndeep mapping fθand reﬁne the cluster centroids by learn-\\ning from current high conﬁdence assignments using an aux-\\niliary target distribution. This process is repeated until a\\nconvergence criterion is met.\\n3.1.1. S OFT ASSIGNMENT\\nFollowing van der Maaten & Hinton (2008) we use the Stu-\\ndent’st-distribution as a kernel to measure the similarity\\nbetween embedded point ziand centroid µj:\\nqij=(1 +∥zi−µj∥2/α)−α+1\\n2\\n∑\\nj′(1 +∥zi−µj′∥2/α)−α+1\\n2, (1)\\nwherezi=fθ(xi)∈Zcorresponds to xi∈Xafter em-\\nbedding,αare the degrees of freedom of the Student’s t-\\ndistribution and qijcan be interpreted as the probability\\nof assigning sample ito clusterj(i.e., a soft assignment).\\nSince we cannot cross-validate αon a validation set in the\\nunsupervised setting, and learning it is superﬂuous (van der\\nMaaten, 2009), we let α= 1for all experiments.\\n3.1.2. KL DIVERGENCE MINIMIZATION\\nWe propose to iteratively reﬁne the clusters by learning\\nfrom their high conﬁdence assignments with the help of\\nan auxiliary target distribution. Speciﬁcally, our model is\\ntrained by matching the soft assignment to the target distri-\\nbution. To this end, we deﬁne our objective as a KL diver-\\ngence loss between the soft assignments qiand the auxil-\\niary distribution pias follows:\\nL= KL(P∥Q) =∑\\ni∑\\njpijlogpij\\nqij. (2)\\nThe choice of target distributions Pis crucial for DEC’s\\nperformance. A naive approach would be setting each pito\\na delta distribution (to the nearest centroid) for data points\\nabove a conﬁdence threshold and ignore the rest. How-\\never, because qiare soft assignments, it is more natural\\nand ﬂexible to use softer probabilistic targets. Speciﬁcally,\\nwe would like our target distribution to have the following\\nproperties: (1) strengthen predictions (i.e., improve clus-\\nter purity), (2) put more emphasis on data points assigned\\nwith high conﬁdence, and (3) normalize loss contributionof each centroid to prevent large clusters from distorting\\nthe hidden feature space.\\nIn our experiments, we compute piby ﬁrst raising qito\\nthe second power and then normalizing by frequency per\\ncluster:\\npij=q2\\nij/fj∑\\nj′q2\\nij′/fj′, (3)\\nwherefj=∑\\niqijare soft cluster frequencies. Please\\nrefer to section 5.1 for discussions on empirical properties\\nofLandP.\\nOur training strategy can be seen as a form of self-\\ntraining (Nigam & Ghani, 2000). As in self-training, we\\ntake an initial classiﬁer and an unlabeled dataset, then la-\\nbel the dataset with the classiﬁer in order to train on its\\nown high conﬁdence predictions. Indeed, in experiments\\nwe observe that DEC improves upon the initial estimate\\nin each iteration by learning from high conﬁdence predic-\\ntions, which in turn helps to improve low conﬁdence ones.\\n3.1.3. O PTIMIZATION\\nWe jointly optimize the cluster centers {µj}and DNN pa-\\nrametersθusing Stochastic Gradient Descent (SGD) with\\nmomentum. The gradients of Lwith respect to feature-\\nspace embedding of each data point ziand each cluster\\ncentroidµjare computed as:\\n∂L\\n∂zi=α+ 1\\nα∑\\nj(1 +∥zi−µj∥2\\nα)−1(4)\\n×(pij−qij)(zi−µj),\\n∂L\\n∂µj=−α+ 1\\nα∑\\ni(1 +∥zi−µj∥2\\nα)−1(5)\\n×(pij−qij)(zi−µj).\\nThe gradients ∂L/∂ziare then passed down to the DNN\\nand used in standard backpropagation to compute the\\nDNN’s parameter gradient ∂L/∂θ . For the purpose of dis-\\ncovering cluster assignments, we stop our procedure when\\nless than tol%of points change cluster assignment between\\ntwo consecutive iterations.\\n3.2. Parameter initialization\\nThus far we have discussed how DEC proceeds given ini-\\ntial estimates of the DNN parameters θand the cluster cen-\\ntroids{µj}. Now we discuss how the parameters and cen-\\ntroids are initialized.\\nWe initialize DEC with a stacked autoencoder (SAE) be-\\ncause recent research has shown that they consistently pro-\\nduce semantically meaningful and well-separated represen-\\ntations on real-world datasets (Vincent et al., 2010; Hin-\\nton & Salakhutdinov, 2006; Le, 2013). Thus the unsuper-', metadata={'source': 'sample_pdf.pdf', 'page': 2}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\nTable 1. Dataset statistics.\\nDataset # Points # classes Dimension % of largest class\\nMNIST (LeCun et al., 1998) 70000 10 784 11%\\nSTL-10 (Coates et al., 2011) 13000 10 1428 10%\\nREUTERS-10K 10000 4 2000 43%\\nREUTERS (Lewis et al., 2004) 685071 4 2000 43%\\nP\\nQ\\nL = KL(P||Q) encoder decoder \\nDEC \\ninput reconstruction \\nfeature \\nFigure 1. Network structure\\nvised representation learned by SAE naturally facilitates\\nthe learning of clustering representations with DEC.\\nWe initialize the SAE network layer by layer with each\\nlayer being a denoising autoencoder trained to reconstruct\\nthe previous layer’s output after random corruption (Vin-\\ncent et al., 2010). A denoising autoencoder is a two layer\\nneural network deﬁned as:\\n˜x∼Dropout (x) (6)\\nh=g1(W1˜x+b1) (7)\\n˜h∼Dropout (h) (8)\\ny=g2(W2˜h+b2) (9)\\nwhere Dropout (·)(Srivastava et al., 2014) is a stochastic\\nmapping that randomly sets a portion of its input dimen-\\nsions to 0,g1andg2are activation functions for encoding\\nand decoding layer respectively, and θ={W1,b1,W2,b2}\\nare model parameters. Training is performed by minimiz-\\ning the least-squares loss ∥x−y∥2\\n2. After training of one\\nlayer, we use its output has the input to train the next\\nlayer. We use rectiﬁed linear units (ReLUs) (Nair & Hin-\\nton, 2010) in all encoder/decoder pairs, except for g2of the\\nﬁrst pair (it needs to reconstruct input data that may have\\npositive and negative values, such as zero-mean images)\\nandg1of the lastpair (so the ﬁnal data embedding retains\\nfull information (Vincent et al., 2010)).\\nAfter greedy layer-wise training, we concatenate all en-\\ncoder layers followed by all decoder layers, in reverse\\nlayer-wise training order, to form a deep autoencoder and\\nthen ﬁnetune it to minimize reconstruction loss. The ﬁnal\\nresult is a multilayer deep autoencoder with a bottleneck\\ncoding layer in the middle. We then discard the decoderlayers and use the encoder layers as our initial mapping be-\\ntween the data space and the feature space, as shown in\\nFig. 1.\\nTo initialize the cluster centers, we pass the data through\\nthe initialized DNN to get embedded data points and then\\nperform standard k-means clustering in the feature space Z\\nto obtainkinitial centroids{µj}k\\nj=1.\\n4. Experiments\\n4.1. Datasets\\nWe evaluate the proposed method (DEC) on one text\\ndataset and two image datasets and compare it against other\\nalgorithms including k-means, LDGMI (Yang et al., 2010),\\nand SEC (Nie et al., 2011). LDGMI and SEC are spec-\\ntral clustering based algorithms that use a Laplacian matrix\\nand various transformations to improve clustering perfor-\\nmance. Empirical evidence reported in Yang et al. (2010);\\nNie et al. (2011) shows that LDMGI and SEC outperform\\ntraditional spectral clustering methods on a wide range of\\ndatasets. We show qualitative and quantitative results that\\ndemonstrate the beneﬁt of DEC compared to LDGMI and\\nSEC.\\nIn order to study the performance and generality of dif-\\nferent algorithms, we perform experiment on two image\\ndatasets and one text data set:\\n•MNIST : The MNIST dataset consists of 70000 hand-\\nwritten digits of 28-by-28 pixel size. The digits are\\ncentered and size-normalized (LeCun et al., 1998).\\n•STL-10 : A dataset of 96-by-96 color images. There\\nare 10 classes with 1300 examples each. It also con-\\ntains 100000 unlabeled images of the same resolu-\\ntion (Coates et al., 2011). We also used the unlabeled\\nset when training our autoencoders. Similar to Doer-\\nsch et al. (2012), we concatenated HOG feature and a\\n8-by-8 color map to use as input to all algorithms.\\n•REUTERS : Reuters contains about 810000 English\\nnews stories labeled with a category tree (Lewis et al.,\\n2004). We used the four root categories: corpo-\\nrate/industrial, government/social, markets, and eco-\\nnomics as labels and further pruned all documents that\\nare labeled by multiple root categories to get 685071', metadata={'source': 'sample_pdf.pdf', 'page': 3}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\n24680.50.550.60.650.70.750.80.850.9Accuracy\\nParameter Index\\nMNIST  DEC DEC w/o backprop Kmeans LDGMI SEC\\n24680.20.250.30.350.4Accuracy\\nParameter Index\\nSTL24680.350.40.450.50.550.60.650.70.75Accuracy\\nParameter Index\\nREUTERS−10K24680.50.550.60.650.70.750.80.85Accuracy\\nParameter Index\\nREUTERS\\nFigure 2. Clustering accuracy for different hyperparameter choices for each algorithm. DEC outperforms other methods and is more\\nrobust to hyperparameter changes compared to either LDGMI or SEC. Robustness is important because cross-validation is not possible\\nin real-world applications of cluster analysis. This ﬁgure is best viewed in color.\\nTable 2. Comparison of clustering accuracy (Eq. 10) on four datasets.\\nMethod MNIST STL-HOG REUTERS-10k REUTERS\\nk-means 53.49% 28.39% 52.42% 53.29%\\nLDMGI 84.09% 33.08% 43.84% N/A\\nSEC 80.37% 30.75% 60.08% N/A\\nDEC w/o backprop 79.82% 34.06% 70.05% 69.62%\\nDEC (ours) 84.30% 35.90% 72.17% 75.63%\\narticles. We then computed tf-idf features on the 2000\\nmost frequently occurring word stems. Since some\\nalgorithms do not scale to the full Reuters dataset,\\nwe also sampled a random subset of 10000 examples,\\nwhich we call REUTERS-10k, for comparison pur-\\nposes.\\nA summary of dataset statistics is shown in Table 1. For\\nall algorithms, we normalize all datasets so that1\\nd∥xi∥2\\n2is\\napproximately 1, where dis the dimensionality of the data\\nspace pointxi∈X.\\n4.2. Evaluation Metric\\nWe use the standard unsupervised evaluation metric and\\nprotocols for evaluations and comparisons to other algo-\\nrithms (Yang et al., 2010). For all algorithms we set the\\nnumber of clusters to the number of ground-truth categories\\nand evaluate performance with unsupervised clustering ac-\\ncuracy ( ACC ):\\nACC = max\\nm∑n\\ni=11{li=m(ci)}\\nn, (10)\\nwhereliis the ground-truth label, ciis the cluster assign-\\nment produced by the algorithm, and mranges over all pos-\\nsible one-to-one mappings between clusters and labels.\\nIntuitively this metric takes a cluster assignment from an\\nunsupervised algorithm and a ground truth assignment and\\nthen ﬁnds the best matching between them. The best map-ping can be efﬁciently computed by the Hungarian algo-\\nrithm (Kuhn, 1955).\\n4.3. Implementation\\nDetermining hyperparameters by cross-validation on a vali-\\ndation set is not an option in unsupervised clustering. Thus\\nwe use commonly used parameters for DNNs and avoid\\ndataset speciﬁc tuning as much as possible. Speciﬁcally,\\ninspired by van der Maaten (2009), we set network dimen-\\nsions tod–500–500–2000–10 for all datasets, where dis\\nthe data-space dimension, which varies between datasets.\\nAll layers are densely (fully) connected.\\nDuring greedy layer-wise pretraining we initialize the\\nweights to random numbers drawn from a zero-mean Gaus-\\nsian distribution with a standard deviation of 0.01. Each\\nlayer is pretrained for 50000 iterations with a dropout rate\\nof20%. The entire deep autoencoder is further ﬁnetuned\\nfor 100000 iterations without dropout. For both layer-wise\\npretraining and end-to-end ﬁnetuning of the autoencoder\\nthe minibatch size is set to 256, starting learning rate is\\nset to 0.1, which is divided by 10 every 20000 iterations,\\nand weight decay is set to 0. All of the above param-\\neters are set to achieve a reasonably good reconstruction\\nloss and are held constant across all datasets. Dataset-\\nspeciﬁc settings of these parameters might improve perfor-\\nmance on each dataset, but we refrain from this type of\\nunrealistic parameter tuning. To initialize centroids, we\\nrunk-means with 20 restarts and select the best solution.', metadata={'source': 'sample_pdf.pdf', 'page': 4}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\n(a) MNIST\\n (b) STL-10\\nFigure 3. Each row contains the top 10 scoring elements from one cluster.\\nIn the KL divergence minimization phase, we train with\\na constant learning rate of 0.01. The convergence thresh-\\nold is set to tol= 0.1%. Our implementation is based\\non Python and Caffe (Jia et al., 2014) and is available at\\nhttps://github.com/piiswrong/dec .\\nFor all baseline algorithms, we perform 20 random restarts\\nwhen initializing centroids and pick the result with the\\nbest objective value. For a fair comparison with previ-\\nous work (Yang et al., 2010), we vary one hyperparameter\\nfor each algorithm over 9 possible choices and report the\\nbest accuracy in Table 2 and the range of accuracies in Fig.\\n2. For LDGMI and SEC, we use the same parameter and\\nrange as in their corresponding papers. For our proposed\\nalgorithm, we vary λ, the parameter that controls annealing\\nspeed, over 2i×10,i= 0,1,...,8. Sincek-means does not\\nhave tunable hyperparameters (aside from k), we simply\\nrun them 9 times. GMMs perform similarly to k-means so\\nwe only report k-means results. Traditional spectral clus-\\ntering performs worse than LDGMI and SEC so we only\\nreport the latter (Yang et al., 2010; Nie et al., 2011).\\n4.4. Experiment results\\nWe evaluate the performance of our algorithm both quan-\\ntitatively and qualitatively. In Table 2, we report the best\\nperformance, over 9 hyperparameter settings, of each al-\\ngorithm. Note that DEC outperforms all other methods,\\nsometimes with a signiﬁcant margin. To demonstrate the\\neffectiveness of end-to-end training, we also show the re-\\nsults from freezing the non-linear mapping fθduring clus-\\ntering. We ﬁnd that this ablation (“DEC w/o backprop”)\\ngenerally performs worse than DEC.In order to investigate the effect of hyperparameters, we\\nplot the accuracy of each method under all 9 settings (Fig.\\n2). We observe that DEC is more consistent across hyper-\\nparameter ranges compared to LDGMI and SEC. For DEC,\\nhyperparameter λ= 40 gives near optimal performance on\\nall dataset, whereas for other algorithms the optimal hyper-\\nparameter varies widely. Moreover, DEC can process the\\nentire REUTERS dataset in half an hour with GPU acceler-\\nation while the second best algorithms, LDGMI and SEC,\\nwould need months of computation time and terabytes of\\nmemory. We, indeed, could not run these methods on the\\nfull REUTERS dataset and report N/A in Table 2 (GPU\\nadaptation of these methods is non-trivial).\\nIn Fig. 3 we show 10 top scoring images from each clus-\\nter in MNIST and STL. Each row corresponds to a cluster\\nand images are sorted from left to right based on their dis-\\ntance to the cluster center. We observe that for MNIST,\\nDEC’s cluster assignment corresponds to natural clusters\\nvery well, with the exception of confusing 4 and 9, while\\nfor STL, DEC is mostly correct with airplanes, trucks and\\ncars, but spends part of its attention on poses instead of\\ncategories when it comes to animal classes.\\n5. Discussion\\n5.1. Assumptions and Objective\\nThe underlying assumption of DEC is that the initial clas-\\nsiﬁer’s high conﬁdence predictions are mostly correct. To\\nverify that this assumption holds for our task and that our\\nchoice ofPhas the desired properties, we plot the mag-\\nnitude of the gradient of Lwith respect to each embedded\\npoint,|∂L/∂zi|, against its soft assignment, qij, to a ran-', metadata={'source': 'sample_pdf.pdf', 'page': 5}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\n(a) Epoch 0\\n (b) Epoch 3\\n (c) Epoch 6\\n(d) Epoch 9\\n (e) Epoch 12\\n0 10 20 30\\n0.790.80.810.820.830.84AccuracyEpochs\\nk-means initialization (f) Accuracy vs. epochs\\nFigure 5. We visualize the latent representation as the KL divergence minimization phase proceeds on MNIST. Note the separation of\\nclusters from epoch 0 to epoch 12. We also plot the accuracy of DEC at different epochs, showing that KL divergence minimization\\nimproves clustering accuracy. This ﬁgure is best viewed in color.\\nTable 3. Comparison of clustering accuracy (Eq. 10) on autoencoder (AE) feature.\\nMethod MNIST STL-HOG REUTERS-10k REUTERS\\nAE+k-means 81.84% 33.92% 66.59% 71.97%\\nAE+LDMGI 83.98% 32.04% 42.92% N/A\\nAE+SEC 81.56% 32.29% 61.86% N/A\\nDEC (ours) 84.30% 35.90% 72.17% 75.63%\\nFigure 4. Gradient visualization at the start of KL divergence min-\\nimization. This plot shows the magnitude of the gradient of the\\nlossLvs. the cluster soft assignment probability qij. See text for\\ndiscussion.\\ndomly chosen MNIST cluster j(Fig. 4).\\nWe observe points that are closer to the cluster center (largeqij) contribute more to the gradient. We also show the raw\\nimages of 10 data points at each 10 percentile sorted by qij.\\nInstances with higher similarity are more canonical exam-\\nples of “5”. As conﬁdence decreases, instances become\\nmore ambiguous and eventually turn into a mislabeled “8”\\nsuggesting the soundness of our assumptions.\\n5.2. Contribution of Iterative Optimization\\nIn Fig. 5 we visualize the progression of the embedded rep-\\nresentation of a random subset of MNIST during training.\\nFor visualization we use t-SNE (van der Maaten & Hinton,\\n2008) applied to the embedded points zi. It is clear that\\nthe clusters are becoming increasingly well separated. Fig.\\n5 (f) shows how accuracy correspondingly improves over\\nSGD epochs.\\n5.3. Contribution of Autoencoder Initialization\\nTo better understand the contribution of each component,\\nwe show the performance of all algorithms with autoen-\\ncoder features in Table 3. We observe that SEC and LD-\\nMGI’s performance do not change signiﬁcantly with au-', metadata={'source': 'sample_pdf.pdf', 'page': 6}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\nTable 4. Clustering accuracy (Eq. 10) on imbalanced subsample of MNIST.XXXXXXXXXXMethodrmin0.1 0.3 0.5 0.7 0.9\\nk-means 47.14% 49.93% 53.65% 54.16% 54.39%\\nAE+k-means 66.82% 74.91% 77.93% 80.04% 81.31%\\nDEC 70.10% 80.92% 82.68% 84.69% 85.41%\\ntoencoder feature, while k-means improved but is still be-\\nlow DEC. This demonstrates the power of deep embedding\\nand the beneﬁt of ﬁne-tuning with the proposed KL diver-\\ngence objective.\\n5.4. Performance on Imbalanced Data\\nIn order to study the effect of imbalanced data, we sample\\nsubsets of MNIST with various retention rates. For mini-\\nmum retention rate rmin, data points of class 0 will be kept\\nwith probability rminand class 9 with probability 1, with\\nthe other classes linearly in between. As a result the largest\\ncluster will be 1/rmintimes as large as the smallest one.\\nFrom Table 4 we can see that DEC is fairly robust against\\ncluster size variation. We also observe that KL divergence\\nminimization (DEC) consistently improves clustering ac-\\ncuracy after autoencoder and k-means initialization (shown\\nas AE+k-means).\\n5.5. Number of Clusters\\n00.20.40.60.81\\n35791113151719Number\\t\\r \\xa0of\\t\\r \\xa0ClustersNMIGeneralizability\\nFigure 6. Selection of the centroid count, k. This is a plot of Nor-\\nmalized Mutual Information (NMI) and Generalizability vs. num-\\nber of clusters. Note that there is a sharp drop of generalizability\\nfrom 9 to 10 which means that 9 is the optimal number of clusters.\\nIndeed, we observe that 9 gives the highest NMI.\\nSo far we have assumed that the number of natural clusters\\nis given to simplify comparison between algorithms. How-\\never, in practice this quantity is often unknown. Therefore\\na method for determining the optimal number of clusters is\\nneeded. To this end, we deﬁne two metrics: (1) the standard\\nmetric, Normalized Mutual Information (NMI), for evalu-ating clustering results with different cluster number:\\nNMI (l,c) =I(l,c)\\n1\\n2[H(l) +H(c)],\\nwhereIis the mutual information metric and His entropy,\\nand (2) generalizability ( G) which is deﬁned as the ratio\\nbetween training and validation loss:\\nG=Ltrain\\nLvalidation.\\nGis small when training loss is lower than validation loss,\\nwhich indicate a high degree of overﬁtting.\\nFig. 6 shows a sharp drop in generalizability when cluster\\nnumber increases from 9 to 10, which suggests that 9 is the\\noptimal number of clusters. We indeed observe the highest\\nNMI score at 9, which demonstrates that generalizability is\\na good metric for selecting cluster number. NMI is highest\\nat 9 instead 10 because 9 and 4 are similar in writing and\\nDEC thinks that they should form a single cluster. This\\ncorresponds well with our qualitative results in Fig. 3.\\n6. Conclusion\\nThis paper presents Deep Embedded Clustering, or DEC—\\nan algorithm that clusters a set of data points in a jointly op-\\ntimized feature space. DEC works by iteratively optimiz-\\ning a KL divergence based clustering objective with a self-\\ntraining target distribution. Our method can be viewed as\\nan unsupervised extension of semisupervised self-training.\\nOur framework provide a way to learn a representation spe-\\ncialized for clustering without groundtruth cluster member-\\nship labels.\\nEmpirical studies demonstrate the strength of our proposed\\nalgorithm. DEC offers improved performance as well as\\nrobustness with respect to hyperparameter settings, which\\nis particularly important in unsupervised tasks since cross-\\nvalidation is not possible. DEC also has the virtue of linear\\ncomplexity in the number of data points which allows it to\\nscale to large datasets.\\n7. Acknowledgment\\nThis work is in part supported by ONR N00014-13-1-0720,\\nNSF IIS- 1338054, and Allen Distinguished Investigator\\nAward.', metadata={'source': 'sample_pdf.pdf', 'page': 7}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\nReferences\\nAggarwal, Charu C and Reddy, Chandan K. Data cluster-\\ning: algorithms and applications . CRC Press, 2013.\\nAlelyani, Salem, Tang, Jiliang, and Liu, Huan. Feature\\nselection for clustering: A review. Data Clustering: Al-\\ngorithms and Applications , 2013.\\nBellman, R. Adaptive Control Processes: A Guided Tour .\\nPrinceton University Press, Princeton, New Jersey, 1961.\\nBengio, Yoshua, Courville, Aaron, and Vincent, Pascal.\\nRepresentation learning: A review and new perspectives.\\n2013.\\nBishop, Christopher M. Pattern recognition and machine\\nlearning . springer New York, 2006.\\nBoutsidis, Christos, Drineas, Petros, and Mahoney,\\nMichael W. Unsupervised feature selection for the k-\\nmeans clustering problem. In NIPS , 2009.\\nCoates, Adam, Ng, Andrew Y , and Lee, Honglak. An\\nanalysis of single-layer networks in unsupervised feature\\nlearning. In International Conference on Artiﬁcial Intel-\\nligence and Statistics , pp. 215–223, 2011.\\nDe la Torre, Fernando and Kanade, Takeo. Discriminative\\ncluster analysis. In ICML , 2006.\\nDoersch, Carl, Singh, Saurabh, Gupta, Abhinav, Sivic,\\nJosef, and Efros, Alexei. What makes paris look like\\nparis? ACM Transactions on Graphics , 2012.\\nGirshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,\\nJitendra. Rich feature hierarchies for accurate object de-\\ntection and semantic segmentation. In CVPR , 2014.\\nHalkidi, Maria, Batistakis, Yannis, and Vazirgiannis,\\nMichalis. On clustering validation techniques. Journal\\nof Intelligent Information Systems , 2001.\\nHinton, Geoffrey E and Salakhutdinov, Ruslan R. Reduc-\\ning the dimensionality of data with neural networks. Sci-\\nence, 313(5786):504–507, 2006.\\nHornik, Kurt. Approximation capabilities of multilayer\\nfeedforward networks. Neural networks , 4(2):251–257,\\n1991.\\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,\\nSergey, Long, Jonathan, Girshick, Ross, Guadarrama,\\nSergio, and Darrell, Trevor. Caffe: Convolutional ar-\\nchitecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093 , 2014.\\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\\nImagenet classiﬁcation with deep convolutional neural\\nnetworks. In NIPS , 2012.Kuhn, Harold W. The hungarian method for the assignment\\nproblem. Naval research logistics quarterly , 2(1-2):83–\\n97, 1955.\\nLe, Quoc V . Building high-level features using large scale\\nunsupervised learning. In Acoustics, Speech and Signal\\nProcessing (ICASSP), 2013 IEEE International Confer-\\nence on , pp. 8595–8598. IEEE, 2013.\\nLeCun, Yann, Bottou, L ´eon, Bengio, Yoshua, and Haffner,\\nPatrick. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE , 86(11):2278–\\n2324, 1998.\\nLewis, David D, Yang, Yiming, Rose, Tony G, and Li, Fan.\\nRcv1: A new benchmark collection for text categoriza-\\ntion research. JMLR , 2004.\\nLi, Tao, Ma, Sheng, and Ogihara, Mitsunori. Entropy-\\nbased criterion in categorical clustering. In ICML , 2004.\\nLiu, Huan and Yu, Lei. Toward integrating feature selection\\nalgorithms for classiﬁcation and clustering. IEEE Trans-\\nactions on Knowledge and Data Engineering , 2005.\\nLong, Jonathan, Shelhamer, Evan, and Darrell, Trevor.\\nFully convolutional networks for semantic segmentation.\\narXiv preprint arXiv:1411.4038 , 2014.\\nMacQueen, James et al. Some methods for classiﬁcation\\nand analysis of multivariate observations. In Proceed-\\nings of the ﬁfth Berkeley symposium on mathematical\\nstatistics and probability , pp. 281–297, 1967.\\nNair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units\\nimprove restricted boltzmann machines. In ICML , 2010.\\nNie, Feiping, Zeng, Zinan, Tsang, Ivor W, Xu, Dong,\\nand Zhang, Changshui. Spectral embedded clustering:\\nA framework for in-sample and out-of-sample spectral\\nclustering. IEEE Transactions on Neural Networks ,\\n2011.\\nNigam, Kamal and Ghani, Rayid. Analyzing the effective-\\nness and applicability of co-training. In Proc. of the ninth\\ninternational conference on Information and knowledge\\nmanagement , 2000.\\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A\\nsimple way to prevent neural networks from overﬁtting.\\nJMLR , 2014.\\nSteinbach, Michael, Ert ¨oz, Levent, and Kumar, Vipin. The\\nchallenges of clustering high dimensional data. In New\\nDirections in Statistical Physics , pp. 273–309. Springer,\\n2004.', metadata={'source': 'sample_pdf.pdf', 'page': 8}),\n",
       " Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\nTian, Fei, Gao, Bin, Cui, Qing, Chen, Enhong, and Liu,\\nTie-Yan. Learning deep representations for graph clus-\\ntering. In AAAI Conference on Artiﬁcial Intelligence ,\\n2014.\\nvan der Maaten, Laurens. Learning a parametric embed-\\nding by preserving local structure. In International Con-\\nference on Artiﬁcial Intelligence and Statistics , 2009.\\nvan Der Maaten, Laurens. Accelerating t-SNE using tree-\\nbased algorithms. JMLR , 2014.\\nvan der Maaten, Laurens and Hinton, Geoffrey. Visualizing\\ndata using t-SNE. JMLR , 2008.\\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Ben-\\ngio, Yoshua, and Manzagol, Pierre-Antoine. Stacked de-\\nnoising autoencoders: Learning useful representations in\\na deep network with a local denoising criterion. JMLR ,\\n2010.\\nV on Luxburg, Ulrike. A tutorial on spectral clustering.\\nStatistics and computing , 2007.\\nXiang, Shiming, Nie, Feiping, and Zhang, Changshui.\\nLearning a mahalanobis distance metric for data cluster-\\ning and classiﬁcation. Pattern Recognition , 2008.\\nXing, Eric P, Jordan, Michael I, Russell, Stuart, and Ng,\\nAndrew Y . Distance metric learning with application to\\nclustering with side-information. In NIPS , 2002.\\nYan, Donghui, Huang, Ling, and Jordan, Michael I. Fast\\napproximate spectral clustering. In ACM SIGKDD ,\\n2009.\\nYang, Yi, Xu, Dong, Nie, Feiping, Yan, Shuicheng, and\\nZhuang, Yueting. Image clustering using local discrim-\\ninant models and global integration. IEEE Transactions\\non Image Processing , 2010.\\nYe, Jieping, Zhao, Zheng, and Wu, Mingrui. Discrimina-\\ntive k-means for clustering. In NIPS , 2008.\\nZeiler, Matthew D and Fergus, Rob. Visualizing and un-\\nderstanding convolutional networks. In ECCV . 2014.', metadata={'source': 'sample_pdf.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"sample_pdf.pdf\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Unsupervised Deep Embedding for Clustering Analysis\\nJunyuan Xie JXIE@CS.WASHINGTON .EDU\\nUniversity of Washington\\nRoss Girshick RBG@FB.COM\\nFacebook AI Research (FAIR)\\nAli Farhadi ALI@CS.WASHINGTON .EDU\\nUniversity of Washington\\nAbstract\\nClustering is central to many data-driven appli-\\ncation domains and has been studied extensively\\nin terms of distance functions and grouping al-\\ngorithms. Relatively little work has focused on\\nlearning representations for clustering. In this\\npaper, we propose Deep Embedded Clustering\\n(DEC), a method that simultaneously learns fea-\\nture representations and cluster assignments us-\\ning deep neural networks. DEC learns a map-\\nping from the data space to a lower-dimensional\\nfeature space in which it iteratively optimizes a\\nclustering objective. Our experimental evalua-\\ntions on image and text corpora show signiﬁcant\\nimprovement over state-of-the-art methods.\\n1. Introduction\\nClustering, an essential data analysis and visualization', metadata={'source': 'sample_pdf.pdf', 'page': 0}),\n",
       " Document(page_content='tool, has been studied extensively in unsupervised machine\\nlearning from different perspectives: What deﬁnes a clus-\\nter? What is the right distance metric? How to efﬁciently\\ngroup instances into clusters? How to validate clusters?\\nAnd so on. Numerous different distance functions and em-\\nbedding methods have been explored in the literature. Rel-\\natively little work has focused on the unsupervised learning\\nof the feature space in which to perform clustering.\\nA notion of distance ordissimilarity is central to data clus-\\ntering algorithms. Distance, in turn, relies on represent-\\ning the data in a feature space. The k-means cluster-\\ning algorithm (MacQueen et al., 1967), for example, uses\\nthe Euclidean distance between points in a given feature\\nspace, which for images might be raw pixels or gradient-\\nProceedings of the 33rdInternational Conference on Machine\\nLearning , New York, NY , USA, 2016. JMLR: W&CP volume', metadata={'source': 'sample_pdf.pdf', 'page': 0}),\n",
       " Document(page_content='48. Copyright 2016 by the author(s).orientation histograms. The choice of feature space is cus-\\ntomarily left as an application-speciﬁc detail for the end-\\nuser to determine. Yet it is clear that the choice of feature\\nspace is crucial; for all but the simplest image datasets,\\nclustering with Euclidean distance on raw pixels is com-\\npletely ineffective. In this paper, we revisit cluster analysis\\nand ask: Can we use a data driven approach to solve for\\nthe feature space and cluster memberships jointly?\\nWe take inspiration from recent work on deep learning for\\ncomputer vision (Krizhevsky et al., 2012; Girshick et al.,\\n2014; Zeiler & Fergus, 2014; Long et al., 2014), where\\nclear gains on benchmark tasks have resulted from learn-\\ning better features. These improvements, however, were\\nobtained with supervised learning, whereas our goal is un-\\nsupervised data clustering. To this end, we deﬁne a pa-\\nrameterized non-linear mapping from the data space Xto', metadata={'source': 'sample_pdf.pdf', 'page': 0}),\n",
       " Document(page_content='a lower-dimensional feature space Z, where we optimize\\na clustering objective. Unlike previous work, which oper-\\nates on the data space or a shallow linear embedded space,\\nwe use stochastic gradient descent (SGD) via backpropaga-\\ntion on a clustering objective to learn the mapping, which\\nis parameterized by a deep neural network. We refer to\\nthis clustering algorithm as Deep Embedded Clustering , or\\nDEC.\\nOptimizing DEC is challenging. We want to simultane-\\nously solve for cluster assignment and the underlying fea-\\nture representation. However, unlike in supervised learn-\\ning, we cannot train our deep network with labeled data.\\nInstead we propose to iteratively reﬁne clusters with an\\nauxiliary target distribution derived from the current soft\\ncluster assignment. This process gradually improves the\\nclustering as well as the feature representation.\\nOur experiments show signiﬁcant improvements over state-\\nof-the-art clustering methods in terms of both accuracy and', metadata={'source': 'sample_pdf.pdf', 'page': 0}),\n",
       " Document(page_content='running time on image and textual datasets. We evaluate\\nDEC on MNIST (LeCun et al., 1998), STL (Coates et al.,arXiv:1511.06335v2  [cs.LG]  24 May 2016', metadata={'source': 'sample_pdf.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=20)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db=FAISS.from_documents(documents,OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1dffbae3d50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for 100000 iterations without dropout. For both layer-wise\\npretraining and end-to-end ﬁnetuning of the autoencoder\\nthe minibatch size is set to 256, starting learning rate is\\nset to 0.1, which is divided by 10 every 20000 iterations,\\nand weight decay is set to 0. All of the above param-\\neters are set to achieve a reasonably good reconstruction\\nloss and are held constant across all datasets. Dataset-\\nspeciﬁc settings of these parameters might improve perfor-\\nmance on each dataset, but we refrain from this type of\\nunrealistic parameter tuning. To initialize centroids, we\\nrunk-means with 20 restarts and select the best solution.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"What is this paper about?\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design ChatPrompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if the user finds the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Introduction\n",
    "## Create Stuff Document Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the authors of the paper are:\\n\\n1. Xing, Eric P\\n2. Jordan, Michael I\\n3. Russell, Stuart\\n4. Ng, Andrew Y\\n\\nThe authors are listed in the reference list at the end of the context passage.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"Who are the authors of the paper?\"})\n",
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
